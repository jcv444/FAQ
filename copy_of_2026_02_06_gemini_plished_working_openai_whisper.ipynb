{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jcv444/FAQ/blob/master/copy_of_2026_02_06_gemini_plished_working_openai_whisper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OpenAI Whisper Notebook"
      ],
      "metadata": {
        "id": "IhzW4btBm-zB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "TpZXzuJtyaeF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 1 - Notebook setup"
      ],
      "metadata": {
        "id": "KieN63M5nP3z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following command will pull and install the latest commit from [OpenAI's Whisper repository](https://github.com/openai/whisper) along with its Python dependencies."
      ],
      "metadata": {
        "id": "X45H_FbRR23b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8KN4hn7RLPU"
      },
      "outputs": [],
      "source": [
        "pip install git+https://github.com/openai/whisper.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You'll also want to set Colab's hardware accelerator to 'GPU'. You can do this by going to 'view resources' (available from the drop-down list next to the RAM/Disk bars) and then selecting 'change runtime type'."
      ],
      "metadata": {
        "id": "IfKosUOTguSw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 2 - High level model access"
      ],
      "metadata": {
        "id": "iox1HLqAnbfH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 - English to English Transcription"
      ],
      "metadata": {
        "id": "6Zyb8Hi1nwH4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this sub-section we'll upload one or more audio files containing English speech and transcribe the content of that audio into English text. So first things first, let's upload the audio:"
      ],
      "metadata": {
        "id": "ueh1lPi5TREW"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2b3a647a"
      },
      "source": [
        "pip install ffmpeg-python"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7be39f7"
      },
      "source": [
        "!apt-get update\n",
        "!apt-get install -y ffmpeg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload() # run this to get an upload widget\n"
      ],
      "metadata": {
        "id": "nxBgZYe-VOvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we'll load Whisper and ask it to transcribe the audio file we just uploaded:"
      ],
      "metadata": {
        "id": "JAcVP1z2S3Vv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload() # run this to get an upload widget[ ]\n",
        "from google.colab import files\n",
        "uploaded = files.upload() # run this to get an upload widget\n"
      ],
      "metadata": {
        "id": "Q5tbvLmmASu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "\n",
        "# Load the Whisper model. You can choose 'tiny.en', 'base.en', 'small.en', 'medium.en', 'large.en'\n",
        "# for English-only transcription, or 'tiny', 'base', 'small', 'medium', 'large' for multilingual models.\n",
        "# Larger models offer better accuracy but require more VRAM and are slower.\n",
        "model = whisper.load_model(\"base.en\")\n",
        "\n",
        "# Dictionary to store transcription results\n",
        "all_transcriptions = {}\n",
        "\n",
        "# Iterate through each uploaded file and transcribe it\n",
        "if uploaded:\n",
        "    print(f\"Processing {len(uploaded)} file(s)...\")\n",
        "    for filename in uploaded.keys():\n",
        "        print(f\"\\nTranscribing '{filename}'...\")\n",
        "        try:\n",
        "            # Transcribe the audio file\n",
        "            # fp16=False is generally safer for accuracy, especially on CPU or if GPU memory is limited.\n",
        "            # Set fp16=True for potentially faster processing on compatible GPUs.\n",
        "            result = model.transcribe(filename, language=\"en\", fp16=False)\n",
        "            transcribed_text = result[\"text\"]\n",
        "            print(f\"Transcription for '{filename}':\\n{transcribed_text}\")\n",
        "            all_transcriptions[filename] = transcribed_text\n",
        "        except Exception as e:\n",
        "            print(f\"Error transcribing '{filename}': {e}\")\n",
        "    print(\"\\nAll transcriptions complete.\")\n",
        "else:\n",
        "    print(\"No files uploaded to transcribe.\")\n",
        "\n",
        "# You can now access all_transcriptions dictionary for results, e.g.:\n",
        "# for filename, text in all_transcriptions.items():\n",
        "#     print(f\"\\n-- {filename} --\\n{text}\")\n"
      ],
      "metadata": {
        "id": "xAzK2Qx8TBHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 French to English Translation"
      ],
      "metadata": {
        "id": "FL6mF-w5tgPG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this sub-section we'll upload one or more audio files containing French speech and translate the content of that audio into English text. Let's upload the audio:"
      ],
      "metadata": {
        "id": "Ojv_9KOutobx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded_french = files.upload() # run this to get an upload widget for French audio\n"
      ],
      "metadata": {
        "id": "fApqDVM8txAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's first see how Whisper fairs transcribing French speech to French text:"
      ],
      "metadata": {
        "id": "N7UToDadwlIr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = whisper.load_model(\"base\")\n",
        "\n",
        "# Assuming you uploaded one French audio file using the previous cell.\n",
        "# If multiple files were uploaded, this will use the first one.\n",
        "if uploaded_french:\n",
        "    french_audio_filename = list(uploaded_french.keys())[0]\n",
        "    print(f\"Transcribing French audio '{french_audio_filename}' to French text...\")\n",
        "    result = model.transcribe(french_audio_filename, language='fr', fp16=False)\n",
        "    print(result[\"text\"])\n",
        "else:\n",
        "    print(\"No French audio file found for transcription. Please upload one in the cell above.\")\n"
      ],
      "metadata": {
        "id": "yAi0qwfxyReQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's see how well it translates French speech to English text:"
      ],
      "metadata": {
        "id": "b7xKoolcraji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = whisper.load_model(\"base\")\n",
        "\n",
        "# Assuming you uploaded one French audio file using the previous cell.\n",
        "# If multiple files were uploaded, this will use the first one.\n",
        "if uploaded_french:\n",
        "    french_audio_filename = list(uploaded_french.keys())[0]\n",
        "    print(f\"Translating French audio '{french_audio_filename}' to English text using 'base' model...\")\n",
        "    result = model.transcribe(french_audio_filename, language='fr', task='translate', fp16=False)\n",
        "    print(result[\"text\"])\n",
        "else:\n",
        "    print(\"No French audio file found for translation. Please upload one in the cell above.\")\n"
      ],
      "metadata": {
        "id": "zPx1Bdmsu5Pn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try the same as above but on a slightly more accurate model:"
      ],
      "metadata": {
        "id": "6qKcGeJwrp6h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = whisper.load_model(\"small\")\n",
        "\n",
        "# Assuming you uploaded one French audio file using the previous cell.\n",
        "# If multiple files were uploaded, this will use the first one.\n",
        "if uploaded_french:\n",
        "    french_audio_filename = list(uploaded_french.keys())[0]\n",
        "    print(f\"Translating French audio '{french_audio_filename}' to English text using 'small' model...\")\n",
        "    result = model.transcribe(french_audio_filename, language='fr', task='translate', fp16=False)\n",
        "    print(result[\"text\"])\n",
        "else:\n",
        "    print(\"No French audio file found for translation. Please upload one in the cell above.\")\n"
      ],
      "metadata": {
        "id": "SXZvBUUPweWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 3 - Low level model access"
      ],
      "metadata": {
        "id": "n_mdCiTqhsoV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we'll look at some low level Whisper access using `whisper.decode()` and `whisper.detect_language()`:"
      ],
      "metadata": {
        "id": "P8KXY4rOngvn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = whisper.load_model('small')\n",
        "\n",
        "# Assuming you uploaded one French audio file using the previous cell.\n",
        "# If multiple files were uploaded, this will use the first one.\n",
        "if uploaded_french:\n",
        "    french_audio_filename = list(uploaded_french.keys())[0]\n",
        "    print(f\"Processing audio '{french_audio_filename}' for low-level access...\")\n",
        "    # load audio and pad/trim it to fit 30 seconds\n",
        "    audio = whisper.load_audio(french_audio_filename)\n",
        "    audio = whisper.pad_or_trim(audio)\n",
        "\n",
        "    # make log-Mel spectrogram and move to the same device as the model\n",
        "    mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
        "else:\n",
        "    print(\"No French audio file found for low-level access examples. Please upload one in the cell above.\")\n"
      ],
      "metadata": {
        "id": "dZGdJyu4nnjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 - Language detection"
      ],
      "metadata": {
        "id": "X5u-SY14-hsH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# detect the spoken language\n",
        "_, probs = model.detect_language(mel)\n",
        "lang = max(probs, key=probs.get)\n",
        "prob = \"{0:.0%}\".format(max(probs.values()))\n",
        "\n",
        "# print language that scored the highest liklihood\n",
        "print(f'Detected language (and probability): {lang}', f'({prob})')"
      ],
      "metadata": {
        "id": "yRVcZYAyqvyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 - French to English Translation"
      ],
      "metadata": {
        "id": "0ak10hLcB3wk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# decode the audio\n",
        "options = whisper.DecodingOptions(language='fr', task='translate')\n",
        "result = whisper.decode(model, mel, options)\n",
        "\n",
        "# print the recognized text\n",
        "print(result.text)"
      ],
      "metadata": {
        "id": "INc50UfBqxgJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}