{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jcv444/FAQ/blob/master/YouTube_Whisper_Transcription.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you're looking at this on GitHub and new to Python Notebooks or Colab, click the Google Colab badge above üëÜ\n",
        "\n",
        "\n",
        "#**Creating YouTube transcripts with OpenAI's Whisper model**\n",
        "\n",
        "üì∫ Getting started video: https://youtu.be/kENRf82_RQs\n",
        "\n",
        "*Colab beginner notes:*\n",
        "<br>\n",
        "1. These files are being loaded on a virtual machine in the cloud. Nothing is being downloaded to your computer (except for the transcript when you click to download it.) When you close this session the instance will be erased.\n",
        "<br>\n",
        "2. The run button is visible when you move your mouse close to the left edge of the code block. It looks kind of like this: ‚ñ∂Ô∏è ...but round...and white on black...so nothing like this. You'll know it when you see it.\n",
        "\n",
        "###**Note: For faster performance set your runtime to \"GPU\"**\n",
        "*Click on \"Runtime\" in the menu and click \"Change runtime type\". Select \"GPU\".*\n",
        "\n",
        "\n",
        "**Step 1.** Follow the instructions in each block and select the options you want\n",
        "<br>\n",
        "**Step 2.** Get the url of the video you want to transcribe\n",
        "<br>\n",
        "**Step 3.** Refresh the folder on the left and download your transcript\n",
        "<br>\n",
        "**Step 4.** Go to your YouTube account and upload the transcript to the video it came from and use \"autosync.\"\n",
        "\n",
        "That's it!\n",
        "\n",
        "Have a question? Hit me up on Twitter:[ @AndrewMayne](https://twitter.com/andrewmayne)\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**What is this?**\n",
        "<br>\n",
        "This is a Python notebook that creates a transcript from a YouTube url using OpenAI's Whisper transcription model that you can then upload to YouTube using the autosync feature to create captions.\n",
        "<br>  \n",
        "**What is OpenAI's Whisper model?**\n",
        "<br>\n",
        "Whisper is an automatic speech recognition (ASR) neural net created by OpenAI that transcribes audio at close to human level.\n",
        "<br>\n",
        "<br>\n",
        "**Why use this?**\n",
        "<br>\n",
        "The quality of the OpenAI Whisper model is amazing (I am slightly biased, but seriously, check it out.) You can also use it to transcribe in other languages.\n",
        "<br>\n",
        "<br>\n",
        "**What do the different model sizes do?**\n",
        "<br>\n",
        "Each model size has an improvement in quality ‚Äì especially with different languages. I've found that for a YouTube video with clear speech, the base model works really well. If you see transcription errors, you can try a larger model.\n",
        "<br>\n",
        "<br>\n",
        "**Do I need timestamps?**\n",
        "<br>\n",
        "Nope. YouTube's autosync function will match the text to the spoken words and syncs up really well. All you need is each spoken sentence in a .txt file.\n",
        "<br>\n",
        "<br>\n",
        "**How do I do this?**\n",
        "<br>\n",
        "Just follow each step. If you've never used Colab of a Python notebook, don't panic. It's super easy and runs in the cloud.\n",
        "<br>\n",
        "<br>\n",
        "**Does this cost anything to use?**\n",
        "<br>\n",
        "Nope. You can use Colab for free and Whisper is an open source model.\n",
        "<br>\n",
        "<br>\n",
        "[Tips for creating a YouTube transcript file](https://support.google.com/youtube/answer/2734799?hl=en)\n",
        "<br>\n",
        "[Information on OpenAI's Whisper model](https://openai.com/blog/whisper/)\n",
        "<br>\n",
        "[OpenAI's Whisper GitHub page](https://github.com/openai/whisper)\n",
        "<br>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Qvz5JoKjwKAu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "1. Click the start button in the upper left side of this block to load the necessary libraries\n",
        "\n",
        "You will need to run this every time you reload this notebook.\n",
        "\"\"\"\n",
        "\n",
        "!pip uninstall -y youtube_dl\n",
        "!pip install yt-dlp\n",
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!sudo apt update && sudo apt install ffmpeg\n",
        "!pip install librosa\n",
        "\n",
        "import whisper\n",
        "import time\n",
        "import librosa\n",
        "import re\n",
        "import yt_dlp"
      ],
      "metadata": {
        "id": "j6svgIwL1a-J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "7d97ceac-1ccf-4d66-b826-abfe3cba6627"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: youtube-dl 2021.12.17\n",
            "Uninstalling youtube-dl-2021.12.17:\n",
            "  Successfully uninstalled youtube-dl-2021.12.17\n",
            "Collecting yt-dlp\n",
            "  Downloading yt_dlp-2026.2.4-py3-none-any.whl.metadata (182 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m182.0/182.0 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading yt_dlp-2026.2.4-py3-none-any.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m73.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: yt-dlp\n",
            "Successfully installed yt-dlp-2026.2.4\n",
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-j_qmzxc6\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-j_qmzxc6\n",
            "  Resolved https://github.com/openai/whisper.git to commit c0d2f624c09dc18e709e37c2ad90c039a4eb72a2\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (10.8.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (2.0.2)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (0.12.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (2.9.0+cu126)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (4.67.2)\n",
            "Requirement already satisfied: triton>=2 in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (3.5.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba->openai-whisper==20250625) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken->openai-whisper==20250625) (2025.11.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken->openai-whisper==20250625) (2.32.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (1.11.1.6)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (2026.1.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->openai-whisper==20250625) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->openai-whisper==20250625) (3.0.3)\n",
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Get:2 https://cli.github.com/packages stable InRelease [3,917 B]\n",
            "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:8 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Fetched 3,917 B in 1s (3,853 B/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "121 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 121 not upgraded.\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.12/dist-packages (0.11.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.12/dist-packages (from librosa) (3.1.0)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: numpy>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from librosa) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.16.3)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.5.3)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.9.0)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.0.0)\n",
            "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (4.15.0)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.1.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from lazy_loader>=0.1->librosa) (26.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa) (4.5.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa) (2.32.4)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile>=0.12.1->librosa) (2.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (3.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2026.1.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "2. Select the model you want to use.\n",
        "\n",
        "Base works really well so it's the default.\n",
        "\n",
        "(For multilingual, remove \".en\" from the model name.)\n",
        "\n",
        "Click the run button after you've made your choice (or left it at default.)\n",
        "\"\"\"\n",
        "\n",
        "# model = whisper.load_model(\"tiny.en\")\n",
        "model = whisper.load_model(\"base.en\")\n",
        "# model = whisper.load_model(\"small.en\")\n",
        "# model = whisper.load_model(\"medium.en\")\n",
        "# model = whisper.load_model(\"large\")"
      ],
      "metadata": {
        "id": "9oRA4UIe104O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4883fffb-14c0-4817-8d42-0dbfa2f279c7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 139M/139M [00:01<00:00, 91.8MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import yt_dlp\n",
        "\"\"\"\n",
        "3. Click the run button and input your YouTube URL in the box below then click enter.\n",
        "\n",
        "You can use this one to test: https://www.youtube.com/watch?v=kENRf82_RQs\n",
        "\n",
        "The video will be loaded and the audio extracted (this is usually the longest part of the process.)\n",
        "\n",
        "Your transcript will appear in the folder on the left (you may have to refresh the folder to see it.)\n",
        "\n",
        "You can download the file when it's completed and upload it on your video's detail page using \"autosync.\"\n",
        "\"\"\"\n",
        "\n",
        "# This will prompt you for a YouTube video URL\n",
        "url = input(\"Enter a YouTube video URL: \")\n",
        "\n",
        "# Create a yt-dlp options dictionary\n",
        "ydl_opts = {\n",
        "    # Specify the format as bestaudio/best\n",
        "    'format': 'bestaudio/best',\n",
        "    # Specify the post-processor as ffmpeg to extract audio and convert to mp3\n",
        "    'postprocessors': [{\n",
        "        'key': 'FFmpegExtractAudio',\n",
        "        'preferredcodec': 'mp3',\n",
        "        'preferredquality': '192',\n",
        "    }],\n",
        "    # Specify the output filename as the video title\n",
        "    'outtmpl': '%(title)s.%(ext)s',\n",
        "}\n",
        "\n",
        "# Download the video and extract the audio\n",
        "with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "    info_dict = ydl.extract_info(url, download=True)\n",
        "    file_path = ydl.prepare_filename(info_dict)\n",
        "\n",
        "# Get the path of the file\n",
        "file_path = file_path.replace('.webm', '.mp3')\n",
        "file_path = file_path.replace('.m4a', '.mp3')\n",
        "\n",
        "# Get the duration\n",
        "duration = librosa.get_duration(filename=file_path)\n",
        "start = time.time()\n",
        "result = model.transcribe(file_path)\n",
        "end = time.time()\n",
        "seconds = end - start\n",
        "\n",
        "print(\"Video length:\", duration, \"seconds\")\n",
        "print(\"Transcription time:\", seconds)\n",
        "\n",
        "# Split result[\"text\"]  on !,? and . , but save the punctuation\n",
        "sentences = re.split(\"([!?.])\", result[\"text\"])\n",
        "\n",
        "# Join the punctuation back to the sentences\n",
        "sentences = [\"\".join(i) for i in zip(sentences[0::2], sentences[1::2])]\n",
        "text = \"\\n\\n\".join(sentences)\n",
        "for s in sentences:\n",
        "  print(s)\n",
        "\n",
        "# Save the file as .txt\n",
        "name = \"\".join(file_path) + \".txt\"\n",
        "with open(name, \"w\") as f:\n",
        "  f.write(text)\n",
        "\n",
        "print(\"\\n\\n\", \"-\"*100, \"\\n\\nYour transcript is here:\", name)"
      ],
      "metadata": {
        "id": "JmbHC2-S33Kl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84b2d3ae-5ac6-4c95-96f3-44bbd0e971ec"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a YouTube video URL: https://www.youtube.com/watch?v=kENRf82_RQs\n",
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=kENRf82_RQs\n",
            "[youtube] kENRf82_RQs: Downloading webpage\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: [youtube] No supported JavaScript runtime could be found. Only deno is enabled by default; to use another runtime add  --js-runtimes RUNTIME[:PATH]  to your command/config. YouTube extraction without a JS runtime has been deprecated, and some formats may be missing. See  https://github.com/yt-dlp/yt-dlp/wiki/EJS  for details on installing one\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[youtube] kENRf82_RQs: Downloading android vr player API JSON\n",
            "[info] kENRf82_RQs: Downloading 1 format(s): 251\n",
            "[download] Destination: Transcribe YouTube videos for free with OpenAI's Whisper.webm\n",
            "[download] 100% of    5.19MiB in 00:00:00 at 5.83MiB/s   \n",
            "[ExtractAudio] Destination: Transcribe YouTube videos for free with OpenAI's Whisper.mp3\n",
            "Deleting original file Transcribe YouTube videos for free with OpenAI's Whisper.webm (pass -k to keep)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2652798481.py:41: FutureWarning: get_duration() keyword argument 'filename' has been renamed to 'path' in version 0.10.0.\n",
            "\tThis alias will be removed in version 1.0.\n",
            "  duration = librosa.get_duration(filename=file_path)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Video length: 368.9769791666667 seconds\n",
            "Transcription time: 18.651933908462524\n",
            " Hi, I'm Andrew and this is a quick tutorial on how to use opening eyes whisper model in Google Colab to make really high quality transcriptions and captions for your YouTube videos.\n",
            " We're starting off here on the GitHub page, which is what links to the Colab notebook.\n",
            " And if you're not familiar with Colab or Jupiter or Python notebooks, any other things I mentioned, don't worry, this will be a very easy tutorial and I'll explain what you need to know as we go along.\n",
            " So you can start off here by seeing this is a copy of the notebook, but we'll just click on that badge, which will take us to the actual notebook running on Google Colab.\n",
            " So what is Google Colab?\n",
            " It's a cloud service that lets you run a Python notebook.\n",
            " What's a Python notebook?\n",
            " A Python notebook is an environment where you can run both Python code and have text that explains what's going on.\n",
            " So I have a text block here describing the notebook.\n",
            " And below that, I have a code block.\n",
            " And when I click on that there, that will actually run the code and you can run each block step by step, which is really helpful for a lot of different kinds of applications and learning how to do things.\n",
            " Now, before running any code in this notebook, what you want to do is click on runtime and make sure that it's using a GPU.\n",
            " That would be a graphic processing unit.\n",
            " Why do we want to do this?\n",
            " Because GPUs work really, really well with a lot of machine learning tasks and it will make it much, much faster.\n",
            " So you can read this whenever you want, but I will walk you through the steps.\n",
            " Step number one is I'm going to click on this button here.\n",
            " It's going to give me a warning saying Google didn't write this.\n",
            " That's correct.\n",
            " I did.\n",
            " And it's going to run.\n",
            " So in the background, it's loading these different libraries.\n",
            " That's the code it's going to need to do what we're asking it to.\n",
            " In this case, YouTube DL, which will help us download the YouTube audio, then it'll have the opening eyes whisper framework.\n",
            " And then it's going to have a couple of things FFA impact in La Broso.\n",
            " And these are the import statements.\n",
            " This is what actually tells it to say, since we've loaded it, now make it available in the code.\n",
            " Now you notice we got a lot of output here.\n",
            " This is telling us each of the steps.\n",
            " Some of the stuff's already installed.\n",
            " Some of it had to be downloaded.\n",
            " Now we don't need to see that.\n",
            " So I can click on that button right there and it will hide it, but it's still there.\n",
            " Every time you run a cell or a block, you'll see a little green checkmark like the one here, since I already ran that one.\n",
            " So we've got the libraries downloaded.\n",
            " Now we need to select a model.\n",
            " So open eyes whisper model comes in five different sizes.\n",
            " And they are extremely capable, even the small ones, but the bigger ones are really useful if you want to translate into different languages.\n",
            " We're going to use the base model because it's really fast and it's high quality.\n",
            " But if you feel like you're not getting the results you want, then you can use a larger model.\n",
            " They will be slower, but more powerful.\n",
            " If you want to use it to translate a different language, then just delete the dot in there.\n",
            " But that's going to be a language we're using large does not have a dot in it's just a general purpose multi language model.\n",
            " So what we do here is we click that and it's going to load the model from the server here.\n",
            " And apparently it's already loaded.\n",
            " So we're good.\n",
            " You would see a little loading indicator if it needed to be loaded, but we're good to go.\n",
            " Next step.\n",
            " Well, next is the actual transcription.\n",
            " So you need a YouTube URL.\n",
            " So I'm going to use one from an old video of mine talking about some writing tip or something.\n",
            " And just click like that on play on run looks like a play button.\n",
            " And you get a little input box there.\n",
            " So I'm going to add my YouTube URL there.\n",
            " And we're going to click enter.\n",
            " And that's going to start the process.\n",
            " And you can see each step along the way.\n",
            " And you can see it running down here.\n",
            " The first thing it has to do, it actually has to load the video from YouTube, which it's doing in the background.\n",
            " And it's funny because it's actually kind of comically slow, even though it's all in the cloud, then it's going to extract the audio.\n",
            " So once it's got the video, it's going to run through a process and just strip out that audio that we can then use that with the model.\n",
            " So now that it has the audio, it says it's downloaded.\n",
            " If I click on this folder icon here, I can see that it has the mp3 file, which tells me that it's now doing the transcription, which it did in the time that it took me to explain that.\n",
            " So this video is over six minutes long to create this transcription.\n",
            " It took under four seconds.\n",
            " And that is by the look of it, a very accurate transcription.\n",
            " And if I go over here, I can click on text.\n",
            " And that will show me my transcription text, which looks good.\n",
            " Now to download this, I just click on the tab here and click download.\n",
            " And it will download for some reason.\n",
            " It can take a long time to upload or download files into Colab notebooks.\n",
            " But other than that, they're pretty awesome.\n",
            " So I now have my text file, which if I go to my YouTube page for that video, I can upload it and use something called auto sync, which will find the right places to start and stop the captions based upon the audio.\n",
            " There is a link here to click on if you want more information about that.\n",
            " But it will literally take you two minutes after you create this transcription to get it uploaded to your YouTube video and have it live, which is pretty awesome.\n",
            " For a comparison, I want to show you what my video looked like before with auto captioning.\n",
            " And then after after we've used the whisper base model to create captions for it.\n",
            " Imagine you're at a grocery store and you put some ice cream in your basket and some old lady grabs it and puts it her own.\n",
            " She does the same with your cereal, your beer, whatever, she keeps taking things from you and put it into her basket.\n",
            " It's kind of crazy.\n",
            " Now you go to a party and you tell your friend this and they're like, that's awesome.\n",
            " And they have you tell to somebody else in some.\n",
            " Imagine you're at a grocery store and you put some ice cream in your basket and some old lady grabs it and puts it her own.\n",
            " She does the same with your cereal, your beer, whatever, she keeps taking things from you and putting it into her basket.\n",
            " It's kind of crazy.\n",
            " Now you go to a party and you tell your friend this and they're like, that's awesome.\n",
            " And they have you tell to somebody else.\n",
            " As you can see, the whisper version has punctuation and gets pretty much every word exactly.\n",
            " And it's a big improvement over auto captioning and took a very little time to do that.\n",
            "\n",
            "\n",
            " ---------------------------------------------------------------------------------------------------- \n",
            "\n",
            "Your transcript is here: Transcribe YouTube videos for free with OpenAI's Whisper.mp3.txt\n"
          ]
        }
      ]
    }
  ]
}