{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jcv444/FAQ/blob/master/Judy's_OpenAI_Whisper_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mrnE34TNUzVa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OpenAI Whisper Notebook"
      ],
      "metadata": {
        "id": "lyXHoe8Hd9sd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Purpose:\n",
        "Create a Google Colab notebook that demonstrates how to use OpenAI's Whisper model for audio transcription and translation, including environment setup, high-level API usage for English and French (Chinese) audio, and low-level model access for language detection and decoding, with detailed explanations for each step.\n"
      ],
      "metadata": {
        "id": "9RG8-YiQTHGM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Workflow\n",
        "1. Install Whisper:\n",
        "  pip install -U openai-whisper\n",
        "1.1  Alternatively, the following command will pull and install the latest commit from this repository, along with its Python dependencies:\n",
        "  pip install git+https://github.com/openai/whisper.git\n",
        "2. Import Whisper from"
      ],
      "metadata": {
        "id": "FaBzvrFM2mkG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 1 - Whisper Notebook Setup"
      ],
      "metadata": {
        "id": "WB5o2pH1eclE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Set Harware Accelerator to GPU:\n",
        "\n",
        "Start by setting Colab's hardware accelerator to 'GPU'. You can do this via either option:\n",
        "1. 'view resources' (available from the drop-down list next to the RAM/Disk bars) and then selecting 'Change runtime type'.\n",
        "2. Top L Runtime > Change runtime type\n",
        "\n"
      ],
      "metadata": {
        "id": "V5rhHCR_RSUF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "dI3XT60GSn05"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#‚¨áÔ∏è Run (Ctrl + Enter) blow Code Cells to execute the COMMANDS‚¨áÔ∏è"
      ],
      "metadata": {
        "id": "SJjSnYZjRhGP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Install Whisper Library (framework) (Run this cell first to set up the environment)\n",
        "The OpenAI Whisper library is essential for this notebook as it provides the core functionality for both audio transcription and translation. This installation step sets up the necessary tools to process audio files and convert spoken language into text, or translate it into another language, using Whisper's powerful models.\n",
        "\n",
        "The following command will add a code cell to pull and install the latest commit from OpenAI's Whisper repository along with its Python dependencies.\n"
      ],
      "metadata": {
        "id": "gNw5Ptj3fPQn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "IdOzfKTyKCJU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installs OpenAI Whisper library from GitHub\n",
        "!pip install git+https://github.com/openai/whisper.git"
      ],
      "metadata": {
        "id": "tMKKG8v6gHQB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3 Install FFmpeg & Then Its Python Wrapper\n",
        "\n",
        "Whisper relies on FFmpeg for efficient audio processing, including loading and manipulating various audio formats. To ensure Whisper functions correctly, FFmpeg must be installed on the system - preferably (highly recommended & often necessary for Python applications that interact with FFmpeg) within a Python wrapper - after FFmpeg has been isntalled. Here's why:  \n",
        "     ‚Ä¢ FFmpeg itself is a command-line tool: It's a powerful standalone program for handling multimedia files. You can use it directly from the terminal (or a Colab code cell with !).   \n",
        "         ‚Ä¢ FFmpeg-python is a Python wrapper: This library provides a more convenient and Pythonic way to build and run FFmpeg commands from within your Python code. Instead of constructing complex shell commands as strings, you can use Python objects and methods to represent FFmpeg operations, making your code cleaner, more readable, and less error-prone.   \n",
        "         So, while you could use ! and manually construct FFmpeg commands, ffmpeg-python makes it much easier to integrate FFmpeg's capabilities into your Python-based workflows, like those often found in Colab notebooks.  "
      ],
      "metadata": {
        "id": "nefEQ02FVegf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.3.1 Install FFmpeg CLI Tool 1st\n",
        "\n",
        "Add a code cell to install the FFmpeg command-line tool: You need to have the actual FFmpeg program installed on the system first. This is done via !sudo apt install ffmpeg (or !apt-get install ffmpeg, **or** apt-get update && apt-get install -y ffmpeg). Without the underlying FFmpeg executable, the Python wrapper won't have anything to control (wrap around).\n",
        "\n"
      ],
      "metadata": {
        "id": "xGQMh8xI95qi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# update the packages & install FFMPEG (Alternative code: `apt-get update && apt-get install -y ffmpeg`. See  below Q&A.)\n",
        "!sudo apt update && sudo apt install ffmpeg"
      ],
      "metadata": {
        "id": "LbUNgbEljpfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.3.2 Install Python Wrapper 2nd\n",
        "Add a code cell to install FFmpeg-python, which is a Python wrapper for FFmpeg. This is highly recommended & often necessary for Python applications to interact with FFmpeg."
      ],
      "metadata": {
        "id": "3Q5qRP3MAj03"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install ffmpeg-python"
      ],
      "metadata": {
        "id": "kstQNkUh4HcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 2 - Upload File & Select Whisper Model"
      ],
      "metadata": {
        "id": "MEBBL8a6QLbA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.1 ‚¨ÜÔ∏è Upload file: .aac,  .flac, .m4a, .mp3, .mp4.ogg, .wav, .wma, but **NOT** Audacity‚Äôs inate .aup files.  \n",
        "2.2 üîÇ Change the file's directory path via R context menu's \"Copy path\" from the file I uploaded in 2.1.   \n",
        "\n",
        "‚õî Do NOT simply enter its file name:\n",
        "```\n",
        "!whisper \"ENTER FILE NAME HERE\" --model base.en\n",
        "```\n",
        "as some Youtubers have erroneously suggested.  \n",
        "2,3 ‚õè Pick from the 5 sizes: Tiny, Base, Small, Medium, & Large.  \n",
        "2.3 ü¶ú Specify the language  \n",
        "2.4 ‚úÖ Run it"
      ],
      "metadata": {
        "id": "TUOOSV5nOLLd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------- Change File's New Directory Path -- --Pick Model --Specify Language\n",
        "!whisper /content/audio2022-11-2823.10.13.wav --model large --language German --task translate\n",
        "\n",
        "# Additional arguments (One Youtuber added below argument in this section, & maybe a good idea to view other arguments here?):\n",
        "!whisper -h"
      ],
      "metadata": {
        "id": "D77bUIWiOOKG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e4b37ae-4b21-430a-baf0-127ede69b913"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: whisper: command not found\n",
            "/bin/bash: line 1: whisper: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f598ff9b"
      },
      "source": [
        "import whisper\n",
        "\n",
        "# Define the path to your audio file\n",
        "audio_file_path = \"/content/audio2022-11-2823.10.13.wav\"\n",
        "\n",
        "# Load the specified Whisper model (e.g., \"large\")\n",
        "# This step loads the model into memory. It is the most time-consuming part, but only happens once.\n",
        "model = whisper.load_model(\"large\")\n",
        "\n",
        "# Perform the transcription and translation\n",
        "# - language='German': Specifies the source language of the audio.\n",
        "# - task='translate': Instructs Whisper to translate the audio content into English.\n",
        "# (Note: Whisper translates to English by default when task='translate' is used)\n",
        "result = model.transcribe(audio_file_path, language='German', task='translate')\n",
        "\n",
        "# Print the translated text\n",
        "print(result[\"text\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7888fe4f"
      },
      "source": [
        "### Crucial Step: Ensure GPU is Active Before Loading Whisper Model\n",
        "\n",
        "For optimal performance and speed with Whisper, especially for larger models or longer audio files, it is **critical** that your Google Colab runtime is set to use a **GPU (Graphics Processing Unit)** *before* the Whisper model is loaded.\n",
        "\n",
        "**How to check/set your runtime to GPU:**\n",
        "1.  Go to the Colab menu: `Runtime` -> `Change runtime type`.\n",
        "2.  In the dialog box, select `GPU` from the `Hardware accelerator` dropdown.\n",
        "3.  Click `Save`.\n",
        "\n",
        "**Why this is important:**\n",
        "When `whisper.load_model()` is executed, the model's complex neural network parameters are loaded onto the specified device. If a GPU is active, the model will load into the GPU's dedicated memory (VRAM), allowing all subsequent transcription and translation computations to leverage the GPU's massive parallel processing power. If the runtime is set to CPU during model loading, the model will load into the CPU's main RAM, leading to significantly slower execution times, even if you switch to GPU later (as the model would need to be reloaded). Make sure this step is completed *before* running the next code cell to load the Whisper model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41258404",
        "outputId": "7080872b-9f41-41ca-b517-46068168ae1e"
      },
      "source": [
        "# Install yt-dlp, which is a powerful tool to download YouTube videos and extract audio.\n",
        "# Use --upgrade to ensure you have the latest version.\n",
        "!pip install --upgrade yt-dlp"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting yt-dlp\n",
            "  Downloading yt_dlp-2026.2.4-py3-none-any.whl.metadata (182 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m182.0/182.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading yt_dlp-2026.2.4-py3-none-any.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: yt-dlp\n",
            "Successfully installed yt-dlp-2026.2.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "35479792",
        "outputId": "565c2c84-02a5-4dc1-cc47-8aaa8b6a674b"
      },
      "source": [
        "import yt_dlp\n",
        "import whisper\n",
        "import os\n",
        "\n",
        "# --- Configuration ---\n",
        "# Replace with your YouTube video URL\n",
        "youtube_url = \"https://www.youtube.com/watch?v=dQw4w9WgXcQ\" # Example URL\n",
        "\n",
        "# Define the output path for the downloaded audio file in Colab\n",
        "output_audio_path = \"/content/youtube_audio.m4a\"\n",
        "\n",
        "# Choose the Whisper model you want to use\n",
        "whisper_model_name = \"base\" # e.g., \"tiny\", \"base\", \"small\", \"medium\", \"large\", \"turbo\"\n",
        "\n",
        "# --- 1. Download audio using yt-dlp ---\n",
        "print(f\"Downloading audio from: {youtube_url}\")\n",
        "\n",
        "ydl_opts = {\n",
        "    'format': 'bestaudio/best',  # Selects the best audio format\n",
        "    'postprocessors': [{\n",
        "        'key': 'FFmpegExtractAudio',  # Post-processor to extract audio\n",
        "        'preferredcodec': 'm4a',      # Preferred audio codec\n",
        "        'preferredquality': '192',    # Preferred audio quality\n",
        "    }],\n",
        "    'outtmpl': output_audio_path, # Output file path template\n",
        "    'noplaylist': True,             # Do not download playlists\n",
        "    'logtostderr': False,           # Suppress non-critical messages to stderr\n",
        "    'quiet': True                   # Suppress all output to stderr\n",
        "}\n",
        "\n",
        "try:\n",
        "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "        info_dict = ydl.extract_info(youtube_url, download=True)\n",
        "    print(f\"Audio downloaded to: {output_audio_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error downloading audio: {e}\")\n",
        "    # Exit if download failed, as Whisper won't have an audio file\n",
        "    exit()\n",
        "\n",
        "# --- 2. Load Whisper model ---\n",
        "print(f\"Loading Whisper model: {whisper_model_name}...\")\n",
        "model = whisper.load_model(whisper_model_name)\n",
        "print(\"Whisper model loaded.\")\n",
        "\n",
        "# --- 3. Transcribe the audio ---\n",
        "print(\"Starting transcription...\")\n",
        "# You can add language and task arguments if needed, e.g., language=\"en\", task=\"translate\"\n",
        "# Using \"auto\" for language detection if not specified.\n",
        "result = model.transcribe(output_audio_path, fp16=False) # fp16=False if using CPU or older GPU\n",
        "\n",
        "print(\"Transcription complete!\")\n",
        "\n",
        "# --- 4. Print the transcription ---\n",
        "print(\"\\n--- Transcription ---\\n\")\n",
        "print(result[\"text\"])\n",
        "\n",
        "# Optionally, remove the downloaded audio file after transcription\n",
        "# Uncomment the line below if you want to clean up storage\n",
        "# os.remove(output_audio_path)\n",
        "# print(f\"Removed downloaded audio: {output_audio_path}\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'whisper'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3569164462.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0myt_dlp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mwhisper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# --- Configuration ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'whisper'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "342b74b3"
      },
      "source": [
        "### Explanation of the YouTube Audio Transcription Code:\n",
        "\n",
        "1.  **`!pip install --upgrade yt-dlp`**: This cell ensures that the `yt-dlp` library is installed and updated to its latest version. `yt-dlp` is a command-line program to download videos from YouTube.com and other video sites.\n",
        "\n",
        "2.  **Importing Libraries**: `yt_dlp` is imported for downloading, `whisper` for transcription, and `os` for potential file operations like cleanup.\n",
        "\n",
        "3.  **Configuration**:\n",
        "    *   `youtube_url`: **Replace the example URL with the actual YouTube link you want to transcribe.**\n",
        "    *   `output_audio_path`: Specifies where the downloaded audio will be saved within your Colab environment (e.g., in the `/content/` directory).\n",
        "    *   `whisper_model_name`: Sets which Whisper model to use (e.g., `base`, `large`, `turbo`).\n",
        "\n",
        "4.  **Downloading Audio (`yt_dlp.YoutubeDL`)**:\n",
        "    *   `ydl_opts`: This dictionary contains options for `yt-dlp`:\n",
        "        *   `'format': 'bestaudio/best'`: Tells `yt-dlp` to select the best available audio stream.\n",
        "        *   `'postprocessors'`: Configures `FFmpeg` (which you should have installed) to extract the audio and convert it to a preferred format (here, `m4a` at `192k` quality).\n",
        "        *   `'outtmpl': output_audio_path`: Sets the name and location of the output audio file.\n",
        "        *   `'quiet': True`: Suppresses most of `yt-dlp`'s console output to keep your notebook clean.\n",
        "    *   The `ydl.extract_info(youtube_url, download=True)` command initiates the download process.\n",
        "\n",
        "5.  **Loading Whisper Model (`whisper.load_model`)**: The chosen Whisper model is loaded into memory. This is the step that greatly benefits from having a GPU enabled in your Colab runtime.\n",
        "\n",
        "6.  **Transcribing Audio (`model.transcribe`)**: The downloaded audio file's path is passed to the loaded Whisper model for transcription. You can add `language` and `task` arguments here if you want to specify the source language or perform translation (e.g., `language=\"en\"`, `task=\"translate\"`).\n",
        "\n",
        "7.  **Printing Results**: The `result[\"text\"]` (the full transcribed text) is printed to the console.\n",
        "\n",
        "8.  **Optional Cleanup (`os.remove`)**: An optional line is included to delete the downloaded audio file after transcription, which can be useful to free up disk space in your Colab environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6svgIwL1a-J",
        "outputId": "f52945df-c854-4185-c6a7-b7755255f1da"
      },
      "source": [
        "\"\"\"\n",
        "1. Click the start button in the upper left side of this block to load the necessary libraries\n",
        "\n",
        "You will need to run this every time you reload this notebook.\n",
        "\"\"\"\n",
        "\n",
        "!pip install yt-dlp\n",
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!sudo apt update && sudo apt install ffmpeg\n",
        "!pip install librosa\n",
        "\n",
        "import whisper\n",
        "import time\n",
        "import librosa\n",
        "import re\n",
        "import yt_dlp"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: yt-dlp in /usr/local/lib/python3.12/dist-packages (2026.2.4)\n",
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-1x0qbabf\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-1x0qbabf\n",
            "  Resolved https://github.com/openai/whisper.git to commit c0d2f624c09dc18e709e37c2ad90c039a4eb72a2\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (10.8.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (2.0.2)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (0.12.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (2.9.0+cpu)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (4.67.2)\n",
            "Collecting triton>=2 (from openai-whisper==20250625)\n",
            "  Downloading triton-3.6.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba->openai-whisper==20250625) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken->openai-whisper==20250625) (2025.11.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken->openai-whisper==20250625) (2.32.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (2025.3.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (2026.1.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->openai-whisper==20250625) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->openai-whisper==20250625) (3.0.3)\n",
            "Downloading triton-3.6.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (188.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m188.3/188.3 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20250625-py3-none-any.whl size=803980 sha256=d2c42dab8939a4f1727c17439c6b419e0bdf95fd4d8fe3c3cec397bae6304003\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-sbcfec89/wheels/c3/03/25/5e0ba78bc27a3a089f137c9f1d92fdfce16d06996c071a016c\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: triton, openai-whisper\n",
            "Successfully installed openai-whisper-20250625 triton-3.6.0\n",
            "Get:1 https://cli.github.com/packages stable InRelease [3,917 B]\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:4 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:6 https://cli.github.com/packages stable/main amd64 Packages [356 B]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:8 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [85.0 kB]\n",
            "Get:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:11 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,724 kB]\n",
            "Get:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease [24.6 kB]\n",
            "Get:13 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,894 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [6,396 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [4,039 kB]\n",
            "Get:16 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [38.8 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,297 kB]\n",
            "Get:18 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy/main amd64 Packages [75.3 kB]\n",
            "Get:19 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [62.6 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,683 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [6,678 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,609 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [70.9 kB]\n",
            "Fetched 37.1 MB in 4s (9,066 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "55 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 55 not upgraded.\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.12/dist-packages (0.11.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.12/dist-packages (from librosa) (3.1.0)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: numpy>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from librosa) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.16.3)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.5.3)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.9.0)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.0.0)\n",
            "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (4.15.0)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.1.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from lazy_loader>=0.1->librosa) (26.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa) (4.5.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa) (2.32.4)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile>=0.12.1->librosa) (2.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (3.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2026.1.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmbHC2-S33Kl"
      },
      "source": [
        "\"\"\"\n",
        "3. Click the run button and input your YouTube URL in the box below then click enter.\n",
        "\n",
        "You can use this one to test: https://www.youtube.com/watch?v=CnT-Na1IeVI\n",
        "\n",
        "The video will be loaded and the audio extracted (this is usually the longest part of the process.)\n",
        "\n",
        "Your transcript will appear in the folder on the left (you may have to refresh the folder to see it.)\n",
        "\n",
        "You can download the file when it's completed and upload it on your video's detail page using \"autosync.\"\n",
        "\"\"\"\n",
        "\n",
        "# This will prompt you for a YouTube video URL\n",
        "url = input(\"Enter a YouTube video URL: \")\n",
        "\n",
        "# Create a youtube-dl options dictionary\n",
        "ydl_opts = {\n",
        "    # Specify the format as bestaudio/best\n",
        "    'format': 'bestaudio/best',\n",
        "    # Specify the post-processor as ffmpeg to extract audio and convert to mp3\n",
        "    'postprocessors': [{\n",
        "        'key': 'FFmpegExtractAudio',\n",
        "        'preferredcodec': 'mp3',\n",
        "        'preferredquality': '192',\n",
        "    }],\n",
        "    # Specify the output filename as the video title\n",
        "    'outtmpl': '%(title)s.%(ext)s',\n",
        "}\n",
        "\n",
        "# Download the video and extract the audio\n",
        "with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "    ydl.download([url])\n",
        "\n",
        "# Get the path of the file\n",
        "file_path = ydl.prepare_filename(ydl.extract_info(url, download=False))\n",
        "file_path = file_path.replace('.webm', '.mp3')\n",
        "file_path = file_path.replace('.m4a', '.mp3')\n",
        "\n",
        "# Get the duration\n",
        "duration = librosa.get_duration(filename=file_path)\n",
        "start = time.time()\n",
        "result = model.transcribe(file_path)\n",
        "end = time.time()\n",
        "seconds = end - start\n",
        "\n",
        "print(\"Video length:\", duration, \"seconds\")\n",
        "print(\"Transcription time:\", seconds)\n",
        "\n",
        "# Split result[\"text\"]  on !,? and . , but save the punctuation\n",
        "sentences = re.split(\"([!?.])\", result[\"text\"])\n",
        "\n",
        "# Join the punctuation back to the sentences\n",
        "sentences = [\"\".join(i) for i in zip(sentences[0::2], sentences[1::2])]\n",
        "text = \"\\n\\n\".join(sentences)\n",
        "for s in sentences:\n",
        "  print(s)\n",
        "\n",
        "# Save the file as .txt\n",
        "name = \"\".join(file_path) + \".txt\"\n",
        "with open(name, \"w\") as f:\n",
        "  f.write(text)\n",
        "\n",
        "print(\"\\n\\n\", \"-\"*100, \"\\n\\nYour transcript is here:\", name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be240dc4"
      },
      "source": [
        "### Explanation of the `import whisper` Code:\n",
        "\n",
        "*   `import whisper`: This line imports the Whisper Python library, making its functions and classes available for use in your code.\n",
        "*   `audio_file_path = \"/content/audio2022-11-2823.10.13.wav\"`: This variable stores the full path to your audio file. Remember to replace this with the actual path to your file, especially if you upload a new one or use a file from Google Drive (e.g., `/content/drive/My Drive/your_audio.m4a`).\n",
        "*   `model = whisper.load_model(\"large\")`: This is the crucial step where the Whisper model named \"large\" is loaded into your computer's memory. This typically takes the longest, especially for larger models, but it only happens once. Once loaded, this `model` object can be used for multiple transcriptions.\n",
        "*   `result = model.transcribe(audio_file_path, language='German', task='translate')`: This line performs the actual transcription and translation.\n",
        "    *   `audio_file_path`: The audio file to process.\n",
        "    *   `language='German'`: Tells Whisper that the spoken language in the audio is German. This helps with better transcription accuracy.\n",
        "    *   `task='translate'`: Instructs Whisper to translate the detected German speech into English text.\n",
        "*   `print(result[\"text\"])`: This line prints the final transcribed and translated text, which is stored in the `\"text\"` key of the `result` dictionary.\n",
        "\n",
        "**Where to place this code:**\n",
        "\n",
        "I recommend placing this code after the installation steps for Whisper and FFmpeg, ideally in a new cell where you begin your audio processing. You can delete the original `!whisper` command cell (cell `D77bUIWiOOKG`) if you intend to use this programmatic approach exclusively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e86460b9"
      },
      "source": [
        "import json\n",
        "\n",
        "# Assuming 'result' dictionary is already available from model.transcribe()\n",
        "# For example:\n",
        "# result = model.transcribe(audio_file_path, language='German', task='translate')\n",
        "\n",
        "# Convert the result dictionary to a JSON string\n",
        "json_output = json.dumps(result, indent=4)\n",
        "\n",
        "# Define the output file path\n",
        "output_json_path = \"/content/transcription_output.json\"\n",
        "\n",
        "# Save the JSON string to a file\n",
        "with open(output_json_path, \"w\") as f:\n",
        "    f.write(json_output)\n",
        "\n",
        "print(f\"JSON output saved to: {output_json_path}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bd262fc9"
      },
      "source": [
        "# Assuming 'result' dictionary is already available from model.transcribe()\n",
        "\n",
        "def format_timestamp(seconds):\n",
        "    # Helper function to format seconds into HH:MM:SS,ms format for SRT/VTT\n",
        "    hours = int(seconds // 3600)\n",
        "    minutes = int((seconds % 3600) // 60)\n",
        "    secs = seconds % 60\n",
        "    milliseconds = int((secs - int(secs)) * 1000)\n",
        "    return f\"{hours:02}:{minutes:02}:{int(secs):02},{milliseconds:03}\"\n",
        "\n",
        "# Generate SRT output\n",
        "srt_content = []\n",
        "for i, segment in enumerate(result[\"segments\"]):\n",
        "    start = format_timestamp(segment[\"start\"])\n",
        "    end = format_timestamp(segment[\"end\"])\n",
        "    text = segment[\"text\"].strip()\n",
        "    srt_content.append(f\"{i + 1}\\n{start} --> {end}\\n{text}\\n\")\n",
        "\n",
        "output_srt_path = \"/content/transcription_output.srt\"\n",
        "with open(output_srt_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"\\n\".join(srt_content))\n",
        "\n",
        "print(f\"SRT output saved to: {output_srt_path}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "309ef0d6"
      },
      "source": [
        "# Assuming 'result' dictionary is already available from model.transcribe()\n",
        "\n",
        "def format_vtt_timestamp(seconds):\n",
        "    # Helper function to format seconds into HH:MM:SS.ms format for VTT\n",
        "    hours = int(seconds // 3600)\n",
        "    minutes = int((seconds % 3600) // 60)\n",
        "    secs = seconds % 60\n",
        "    milliseconds = int((secs - int(secs)) * 1000)\n",
        "    return f\"{hours:02}:{minutes:02}:{int(secs):02}.{milliseconds:03}\"\n",
        "\n",
        "# Generate VTT output\n",
        "vtt_content = [\"WEBVTT\\n\"]\n",
        "for segment in result[\"segments\"]:\n",
        "    start = format_vtt_timestamp(segment[\"start\"])\n",
        "    end = format_vtt_timestamp(segment[\"end\"])\n",
        "    text = segment[\"text\"].strip()\n",
        "    vtt_content.append(f\"{start} --> {end}\\n{text}\\n\")\n",
        "\n",
        "output_vtt_path = \"/content/transcription_output.vtt\"\n",
        "with open(output_vtt_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"\\n\".join(vtt_content))\n",
        "\n",
        "print(f\"VTT output saved to: {output_vtt_path}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33611e49"
      },
      "source": [
        "# Assuming 'result' dictionary is already available from model.transcribe()\n",
        "\n",
        "# The full text is directly available in result[\"text\"]\n",
        "text_content = result[\"text\"]\n",
        "\n",
        "output_txt_path = \"/content/transcription_output.txt\"\n",
        "with open(output_txt_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(text_content.strip())\n",
        "\n",
        "print(f\"TXT output saved to: {output_txt_path}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f332d638"
      },
      "source": [
        "### Explanation for Generating Output Files:\n",
        "\n",
        "*   **JSON Output:** The `json` module is used to convert the Python `result` dictionary directly into a JSON formatted string, which is then saved to a `.json` file. The `indent=4` argument makes the JSON human-readable.\n",
        "\n",
        "*   **SRT Output:** This code iterates through each `segment` in the `result['segments']` list. A helper function `format_timestamp` is used to convert seconds into the `HH:MM:SS,ms` format required by SRT. Each segment is then formatted as an SRT block (sequence number, timestamp line, text line) and joined with newlines.\n",
        "\n",
        "*   **VTT Output:** Similar to SRT, but starts with `WEBVTT` and uses `HH:MM:SS.ms` timestamp format. A separate `format_vtt_timestamp` function handles this.\n",
        "\n",
        "*   **TXT Output:** This is the simplest; it just takes the complete `result['text']` string and writes it directly to a `.txt` file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0042d1fd"
      },
      "source": [
        "# Assuming 'result' dictionary is already available from model.transcribe()\n",
        "\n",
        "# Generate TSV output\n",
        "tsv_content = [\"start\\tend\\ttext\"]\n",
        "for segment in result[\"segments\"]:\n",
        "    start = segment[\"start\"]\n",
        "    end = segment[\"end\"]\n",
        "    text = segment[\"text\"].strip()\n",
        "    tsv_content.append(f\"{start}\\t{end}\\t{text}\")\n",
        "\n",
        "output_tsv_path = \"/content/transcription_output.tsv\"\n",
        "with open(output_tsv_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"\\n\".join(tsv_content))\n",
        "\n",
        "print(f\"TSV output saved to: {output_tsv_path}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16973343"
      },
      "source": [
        "### Explanation for Generating TSV Output:\n",
        "\n",
        "*   **TSV Output:** This code iterates through each `segment` in the `result['segments']` list. It creates a header row (`start\\tend\\ttext`) and then formats each segment's start time, end time, and transcribed text, separating them with tabs (`\\t`). Each segment forms a new line, creating a Tab-Separated Values file."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 3 Force\n"
      ],
      "metadata": {
        "id": "aobkCKqE2Rzm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 4 - Mount Google Drive for Persistent Storage\n",
        "\n",
        "Mounting Google Drive is highly beneficial for this notebook as it provides persistent storage for audio files and model outputs. Instead of re-uploading files every session, you can store them directly on your Google Drive and access them from Colab. This also allows you to save any transcribed text or translated results directly to your Drive. When referencing files from Google Drive, use paths like `/content/drive/My Drive/your_audio.m4a`.\n",
        "\n",
        "The next step is to add a code cell to mount Google Drive, which should come after the text cell explaining Google Drive mounting."
      ],
      "metadata": {
        "id": "Z1wtjh2PnChF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "eau510qFv3n8",
        "outputId": "b0f9fe77-84d8-4848-99fb-ae49342c480c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1408506528.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    132\u001b[0m   )\n\u001b[1;32m    133\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 5 - High Level Model Access"
      ],
      "metadata": {
        "id": "Rgh3ZQxsiAmx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.1 - English to English Transcription\n",
        "\n",
        "In this sub-section we'll upload one or more audio files containing English speech and transcribe the content of that audio into English text. So first things first, let's upload the audio:"
      ],
      "metadata": {
        "id": "iYgZsbIDjNs-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload() # run this to get an upload widget"
      ],
      "metadata": {
        "id": "_AVerHLSj2nQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we'll load Whisper and ask it to transcribe the audio file we just uploaded:"
      ],
      "metadata": {
        "id": "3b_aRnpEkTEp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "\n",
        "model = whisper.load_model(\"base.en\")\n",
        "result = model.transcribe(\"eleanor_oliphant_long.m4a\", language=\"en\", fp16=False)\n",
        "print(result[\"text\"])"
      ],
      "metadata": {
        "id": "74tfiQX-oILr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.2 French to English Translation\n",
        "\n",
        "In this sub-section we'll upload one or more audio files containing French speech and translate the content of that audio into English text. Let's upload the audio:"
      ],
      "metadata": {
        "id": "czeGpMYckmm7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload() # run this to get an upload widget"
      ],
      "metadata": {
        "id": "SjxYCmuVlFVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's first see how Whisper fairs transcribing French speech to French text:"
      ],
      "metadata": {
        "id": "Khyr5w-ZlHss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = whisper.load_model(\"base\")\n",
        "result = model.transcribe(\"amelie_original.m4a\", language='fr', fp16=False)\n",
        "print(result[\"text\"])"
      ],
      "metadata": {
        "id": "xSUl3js4lXXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's see how well it translates French speech to English text:"
      ],
      "metadata": {
        "id": "pq9UiokYnYCo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = whisper.load_model(\"base\")\n",
        "result = model.transcribe(\"amelie_original.m4a\", language='fr', task='translate', fp16=False)\n",
        "print(result[\"text\"])"
      ],
      "metadata": {
        "id": "JMJ1n-DxnrS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try the same as above but on a slightly more accurate model:"
      ],
      "metadata": {
        "id": "8et--HhxpT53"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = whisper.load_model(\"small\")\n",
        "result = model.transcribe(\"amelie_original.m4a\", language='fr', task='translate', fp16=False)\n",
        "print(result[\"text\"])"
      ],
      "metadata": {
        "id": "DQU5dlzxpgz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 6 - Low Level Model Access\n",
        "\n",
        "Below we'll look at some low level Whisper access using `whisper.decode()` and `whisper.detect_language()`:"
      ],
      "metadata": {
        "id": "7_hGYhwKzEnD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = whisper.load_model('small')\n",
        "\n",
        "# load audio and pad/trim it to fit 30 seconds\n",
        "audio = whisper.load_audio('amelie_original.m4a')\n",
        "audio = whisper.pad_or_trim(audio)\n",
        "\n",
        "# make log-Mel spectrogram and move to the same device as the model\n",
        "mel = whisper.log_mel_spectrogram(audio).to(model.device)"
      ],
      "metadata": {
        "id": "P2pF5IsgsZ_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.1 - Language Detection"
      ],
      "metadata": {
        "id": "FtnWZiXtzJDL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# detect the spoken language\n",
        "_, probs = model.detect_language(mel)\n",
        "lang = max(probs, key=probs.get)\n",
        "prob = \"{0:.0%}\".format(max(probs.values()))\n",
        "\n",
        "# print language that scored the highest liklihood\n",
        "print(f'Detected language (and probability): {lang}', f'({prob})')"
      ],
      "metadata": {
        "id": "Q39MHBRvs_j8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.2 - French to English Translation"
      ],
      "metadata": {
        "id": "KoRIKFJltLO1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# decode the audio\n",
        "options = whisper.DecodingOptions(language='fr', task='translate')\n",
        "result = whisper.decode(model, mel, options)\n",
        "\n",
        "# print the recognized text\n",
        "print(result.text)"
      ],
      "metadata": {
        "id": "v-wOI30EtT5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Below Are Clarification Q&A."
      ],
      "metadata": {
        "id": "CxUU2zz7vQ39"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. .ipynb File Extension\n",
        "**Q:** What does the I in .ipynb stand for?\n",
        "**A:** The 'I' in .ipynb stands for IPython. This is because Jupyter notebooks evolved from the IPython project, which was originally focused on providing an interactive Python shell. While Jupyter now supports many kernels (Julia, Python, R, etc.), the file extension ipynb remains as a legacy of its IPython origins."
      ],
      "metadata": {
        "id": "pD4mIG9k4X97"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H_TntMvT4XL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Diff b/w These 2 Codes:\n",
        "```\n",
        "!sudo¬†apt¬†update¬†&&¬†sudo¬†apt¬†install¬†ffmpeg)¬†vs\n",
        "apt-get¬†update¬†&&¬†apt-get¬†install¬†-y¬†ffmpeg')\n",
        "```\n",
        "\n",
        "**A:** Both code snippets aim to install FFmpeg on your Colab environment. Here's a breakdown of the differences:\n",
        "    1. !sudo apt update && sudo apt install ffmpeg\n",
        "            ‚ó¶ !: This is an [redacted link] command, commonly used in Colab, that executes the rest of the line as a shell command.\n",
        "                    ‚ó¶ sudo: Stands for \"superuser do.\" It grants administrative privileges to execute the command. While often used out of habit, it's frequently not strictly necessary for package installations in Colab, as you often have sufficient permissions.\n",
        "                            ‚ó¶ apt: This is the newer, more user-friendly command-line utility for package management in Debian-based systems (like the one Colab uses).\n",
        "                                    ‚ó¶ update: Refreshes the list of available packages from the repositories.\n",
        "                                            ‚ó¶ &&: A shell operator that means the second command (sudo apt install ffmpeg) will only execute if the first command (sudo apt update) successfully completes.\n",
        "                                                    ‚ó¶ install ffmpeg: Installs the FFmpeg package.\n",
        "                                                        2. get_ipython().system('apt-get update && apt-get install -y ffmpeg')\n",
        "                                                                ‚ó¶ get_ipython().system(): This is the programmatic way to execute shell commands within an IPython environment (like Colab). It achieves the same outcome as the ! magic command.\n",
        "                                                                        ‚ó¶ apt-get: This is an older, more verbose command-line tool for package management, still widely used in scripts. Functionally, for update and install, apt-get is very similar to apt.\n",
        "                                                                                ‚ó¶ -y: This flag automatically answers \"yes\" to any prompts during the installation process, making the operation non-interactive. While useful in scripts, Colab environments often handle prompts gracefully even without this flag.\n",
        "                                                                                In summary:\n",
        "                                                                                Both commands will successfully install FFmpeg. The first one using ! and apt is generally more concise and preferred in modern interactive environments like Colab. The second one using get_ipython().system() and apt-get -y is a more explicit, script-like way of achieving the same, often seen in older documentation or scripts where non-interactive execution was critical.\n",
        "                                                                                For practical purposes in Colab, !apt update && !apt install ffmpeg (or combining them with && as in your example) is often the simplest and most readable approach.\n",
        "\n",
        "Building on my previous explanation, the core differences are:\n",
        "    ‚Ä¢ !sudo apt update && sudo apt install ffmpeg\n",
        "            ‚ó¶ Uses the ! IPython magic command for shell execution.\n",
        "                    ‚ó¶ Employs the newer apt package manager.\n",
        "                            ‚ó¶ sudo is often included, granting superuser privileges.\n",
        "                                ‚Ä¢ get_ipython().system('apt-get update && apt-get install -y ffmpeg')\n",
        "                                        ‚ó¶ Uses the more explicit get_ipython().system() function for shell execution.\n",
        "                                                ‚ó¶ Employs the slightly older apt-get package manager.\n",
        "                                                        ‚ó¶ Includes the -y flag for non-interactive installation (automatically answers 'yes' to prompts).\n",
        "                                                        Both achieve the same goal of installing FFmpeg in a Colab environment. For general use in Colab, the first syntax (!sudo apt...) is generally more common due to its brevity."
      ],
      "metadata": {
        "id": "gwEcUxI12HuM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Does FFMPEG Need a Python Wrapper?\n",
        "\n",
        "Diff b/w these 2 codes:\n",
        "```\n",
        "pip install ffmpeg-python vs\n",
        "!sudo apt update && sudo apt install ffmpeg\n",
        "```\n",
        "Which code is to install the wrapper? Does the wrapper need to be installed 1st?  \n",
        "\n",
        "**A:** That's a great set of clarifying questions!  \n",
        "1. Does FFmpeg need a Python wrapper?\n",
        "Not strictly need, but it's highly recommended and often necessary for Python applications that interact with FFmpeg. Here's why:\n",
        "    ‚Ä¢ FFmpeg itself is a command-line tool: It's a powerful standalone program for handling multimedia files. You can use it directly from the terminal (or a Colab code cell with !).   \n",
        "        ‚Ä¢ ffmpeg-python is a Python wrapper: This library provides a more convenient and Pythonic way to build and run FFmpeg commands from within your Python code. Instead of constructing complex shell commands as strings, you can use Python objects and methods to represent FFmpeg operations, making your code cleaner, more readable, and less error-prone.    \n",
        "        So, while you could use ! and manually construct FFmpeg commands, ffmpeg-python makes it much easier to integrate FFmpeg's capabilities into your Python-based workflows, like those often found in Colab notebooks.  \n",
        "        2. Diff b/w these 2 codes:   \n",
        "            ‚Ä¢ pip install ffmpeg-python  \n",
        "                    ‚ó¶ This command uses pip, the Python package installer. Its purpose is to install Python libraries into your Python environment.  \n",
        "                            ‚ó¶ It installs the ffmpeg-python library, which is the Python wrapper mentioned above.  \n",
        "                            This library does not install the FFmpeg command-line tool itself.  \n",
        "                                ‚Ä¢ !sudo apt update && sudo apt install ffmpeg     \n",
        "                                        ‚ó¶ This command uses apt (or apt-get), which is a system-level package manager for Debian-based Linux distributions (like the one Colab uses).      \n",
        "                                                ‚ó¶ Its purpose is to install system applications and libraries, including command-line tools like FFmpeg.   \n",
        "                                                        ‚ó¶ This command installs the actual FFmpeg executable (the command-line tool) that ffmpeg-python will then interact with.   \n",
        "                                                        3. Which code is to install the wrapper?  \n",
        "                                                        pip install ffmpeg-python installs the Python wrapper.  \n",
        "                                                        4. Does the wrapper need to be installed 1st?   \n",
        "                                                        No, the wrapper does not need to be installed first. In fact, it's typically the other way around:  \n",
        "                                                            1. Install the FFmpeg command-line tool: You need to have the actual FFmpeg program installed on the system first. This is done via !sudo apt install ffmpeg (or !apt-get install ffmpeg). Without the underlying FFmpeg executable, the Python wrapper won't have anything to control.    \n",
        "                                                                2. Install the Python wrapper (ffmpeg-python): Once FFmpeg is available on the system, you can then install the Python wrapper via !pip install ffmpeg-python to easily interact with it from your Python code.   \n",
        "                                                                So, the correct order is to install the system-level FFmpeg, then the ffmpeg-python wrapper if you plan to use it in your Python code.  \n",
        "                                                                === === ===  === === ===    \n",
        "                                                                What would happen if the Python wrapper is installed before the FFmpeg?   \n",
        "                                                                --- --- ---     \n",
        "                                                                If you install the ffmpeg-python wrapper before installing the actual FFmpeg command-line tool, the Python wrapper won't be able to function correctly. The ffmpeg-python library is essentially a translator between your Python code and the FFmpeg executable.  \n",
        "                                                                When you try to use ffmpeg-python (e.g., to process an audio file), it will look for the ffmpeg executable on your system's PATH. If ffmpeg isn't installed, the Python wrapper won't find it, and you'll typically encounter errors like FileNotFoundError or similar messages indicating that the ffmpeg command cannot be found. The wrapper needs the underlying tool to do the heavy lifting of audio/video processing.  \n",
        "                                                                So, even though the ffmpeg-python library itself would be installed in your Python environment, it would be effectively useless until the ffmpeg command-line tool is also installed on the system.  \n",
        "                                                                === === ===  === === ===     \n",
        "                                                                But if I installed the Ffmpeg after the Python wrapper, would they find each other?   \n",
        "                                                                --- --- ---    \n",
        "                                                                Yes, if you install the ffmpeg-python wrapper first, and then install the FFmpeg command-line tool, they will find each other as long as FFmpeg is installed correctly and its executable is discoverable by your system's PATH.  \n",
        "\n",
        "                                                                Here's why:  \n",
        "\n",
        "                                                                    When you pip install ffmpeg-python, you're installing the Python library itself. This doesn't involve checking for the FFmpeg executable at installation time.  \n",
        "                                                                        When you later run !sudo apt install ffmpeg, the FFmpeg executable is installed in a standard system location (e.g., /usr/bin/ffmpeg) which is typically included in the system's PATH environment variable.  \n",
        "                                                                            When your Python code then tries to use ffmpeg-python to interact with FFmpeg, the ffmpeg-python library internally looks for the ffmpeg executable by searching the directories specified in the system's PATH. If it finds it, it will use it.   \n",
        "\n",
        "                                                                            So, the order of installation for the Python wrapper versus the system-level FFmpeg executable doesn't matter as much as ensuring that both are installed and that the FFmpeg executable is accessible via the system's PATH when ffmpeg-python tries to invoke it."
      ],
      "metadata": {
        "id": "Ypy29SeBBxFw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Enter the File's Directory Path & NOT the File Name\n",
        "**Q:** I am confused on what I should enter into spaces b/w the double quotes ‚Äú ‚Äú for below code. Should I enter the file name or the path via the audio file‚Äôs R context menu?   \n",
        "```\n",
        "# ------- Change File's New Directory Path -- --Pick Model --Specify Language\n",
        "!whisper /content/audio2022-11-2823.10.13.wav --model large --language German --task translate\n",
        "vs  \n",
        "!whisper \"ENTER FILE NAME HERE\" --model base.en  \n",
        "\n",
        "```\n",
        "\n",
        "**A:** That's a very common point of confusion! For the !whisper command, you should enter the full path to your audio file inside the double quotes.  \n",
        "The instruction in Section 2, specifically \"2.2 üîÇ Change the directory via R context menu's \"Copy path\" from the file I uploaded in 2.1. !whisper \"ENTER FILE NAME HERE\" --model base.en\", guides you to do exactly that.  \n",
        "So, after you've uploaded your audio file to Colab (e.g., to /**content**/), you'll right-click on it in the Colab file browser, select \"Copy path\", and then paste that full path into the double quotes. For example, if your file is named my_audio.wav and it's in the **root** /**content**/ directory, the path would be /**content**/my_audio.wav.  \n",
        "Therefore, your command would look like this:  \n",
        "!whisper¬†\"/**content**/my_audio.wav\"¬†--model¬†base.en  \n",
        "Or, if you've mounted Google Drive and your file is there, it might look like:\n",
        "!whisper¬†\"/**content**/drive/My¬†Drive/my_audio_folder/my_audio.m4a\"¬†--model¬†base.en   \n",
        "Always use the path obtained from \"Copy path\" to ensure it's correct."
      ],
      "metadata": {
        "id": "P9VXNqMYcItY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Large vs large for Model Size Code Flag\n",
        "\n",
        "**Q:** Does it matter if I entered the code for model size with small case vs mixed cases?  \n",
        "```\n",
        "--model large vs\n",
        "--model Large\n",
        "```\n",
        "\n",
        "**A:** No, it does not matter if you enter the model size with a lowercase (--model large) or mixed case (--model Large). The Whisper command-line tool, specifically for the --model argument, is generally case-insensitive for these model names.\n",
        "So, both large and Large (or even LARGE) will correctly instruct Whisper to load the 'large' model. You can use either form, and the command will function as expected.  "
      ],
      "metadata": {
        "id": "srMcYLwiuoz4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Case-Sensitive Code Word(s)\n",
        "**Q:** Which, if any, command-line word(s) are case-sensitive?  \n",
        "**A:** Generally, command-line arguments are case-sensitive unless the tool's developers have specifically designed them to be case-insensitive for certain arguments. For example, whisper and Whisper are likely to be treated as different commands if both existed, but usually, the primary command is defined in a specific case.  \n",
        "Regarding the arguments after the whisper command:  \n",
        "    ‚Ä¢ As we discussed, the --model argument is case-insensitive for its values (e.g., large, Large, LARGE all work).   \n",
        "        ‚Ä¢ Flags like -h (for help) are typically case-sensitive in their short form; you wouldn't use -H for the same purpose unless explicitly defined.   \n",
        "            ‚Ä¢ Other arguments, such as --language or --task, and their respective values (German, transcribe, translate) are generally expected to be case-sensitive. For instance, german might not be recognized if the tool expects German.   \n",
        "            To be absolutely sure about any specific argument, the best approach is always to consult the tool's help documentation by running !whisper -h. The help output usually provides the exact spelling and expected case for each argument and its possible values."
      ],
      "metadata": {
        "id": "qDjM5BPUyPNf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. When to Force Reinstall Whisper\n",
        "\n",
        "**Q:** What type of issues when running Whisper would prompt me to resort to this potential fix? !pip install git+https://github.com/openai/whisper.git --force-reinstall\n",
        "You would typically resort to !pip install git+https://github.com/openai/whisper.git --force-reinstall when encountering specific, stubborn issues that suggest a problem with your Whisper installation itself, or a conflict that a standard upgrade or install isn't resolving. Here are the types of issues that might prompt this fix:\n",
        "    1. Corrupted Installation: If you suspect that your Whisper installation somehow became corrupted (e.g., files were accidentally deleted, modified, or an incomplete installation occurred). This can manifest as ModuleNotFoundError for Whisper's internal components, unexpected crashes, or strange behavior that doesn't seem to be related to your code or input.\n",
        "        2. Persistent Version Conflicts (especially with main branch installs): If you previously installed Whisper from the main branch of the GitHub repository, and then later updated some other dependencies, it's possible that pip might have left some older Whisper files that are now incompatible. A --force-reinstall ensures that everything related to Whisper is removed and then freshly installed, resolving potential file-level conflicts.\n",
        "            3. Changes in the main branch that pip isn't detecting for upgrade: Sometimes, pip might not correctly detect that the version on the GitHub main branch is actually \"newer\" than what it has locally, even if new commits have been pushed. This can happen with direct git+ installs because pip's version comparison logic can be tricky with non-standard versioning from Git. --force-reinstall bypasses this check and just reinstalls.\n",
        "                4. Debugging and Ensuring the Latest Code: If you're actively debugging an issue that you know has been very recently fixed in the GitHub repository (e.g., someone just pushed a fix a few hours ago), and a regular pip install --upgrade ... isn't pulling that specific commit, --force-reinstall guarantees you're getting the absolute latest code directly from the repository.\n",
        "                    5. Environment Peculiarities: In some complex or highly customized environments (less common in Colab, but can happen), pip might behave unexpectedly. A --force-reinstall can act as a \"reset button\" for a specific package's installation.\n",
        "                    When NOT to use it:\n",
        "                        ‚Ä¢ For simple upgrades: If you just want to update to the latest version, !pip install --upgrade git+https://github.com/openai/whisper.git is usually sufficient and less aggressive.\n",
        "                            ‚Ä¢ If you suspect dependency issues: --force-reinstall primarily targets the Whisper package itself. If the problem is with a dependency of Whisper (e.g., an incompatible torch version), you'd need to address that specific dependency separately, or use a command that allows pip to resolve and reinstall dependencies (i.e., without --no-deps).\n",
        "                            In summary, !pip install git+https://github.com/openai/whisper.git --force-reinstall is a powerful tool to ensure a clean, fresh installation of Whisper from its GitHub source, but it should be used when other, less aggressive update methods haven't resolved an issue, or when you specifically need to guarantee the freshest possible installation."
      ],
      "metadata": {
        "id": "5Ga3khE3gg8E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7.1 No Need to Update or Upgrade when running the --force-reinstall.\n",
        "\n",
        "**Q:** Should I update or upgrade 1st when I run the\n",
        "!pip install git+https://github.com/openai/whisper.git ‚Äìforce-reinstall\n",
        "\n",
        "If yes, when should I?\n",
        "\n",
        "**A:** That's a good question for understanding pip's behavior!\n",
        "When you use !pip install git+https://github.com/openai/whisper.git --force-reinstall, you do not need to run a separate update or upgrade command first.  \n",
        "Here's why:  \n",
        "    ‚Ä¢ The --force-reinstall flag tells pip to completely remove any existing installation of the specified package (Whisper in this case) and then install it fresh from the provided source (the GitHub repository). This inherently includes the latest version available from that source.  \n",
        "        ‚Ä¢ The --upgrade flag (which is often implied or explicitly used in other scenarios) is also effectively covered because you are forcing a new installation of the current state of the repository, which by definition is the most \"upgraded\" version available from that source at that moment.   \n",
        "        So, the command !pip install git+https://github.com/openai/whisper.git --force-reinstall is a standalone instruction that achieves both the removal of an old installation and the installation of the very latest version from the GitHub main branch. It's a comprehensive 'reset and install latest' command for that specific package.  \n",
        "        When should you use it?  \n",
        "        As we discussed earlier, you should use this command when:  \n",
        "            ‚Ä¢ You suspect your current Whisper installation is corrupted or behaving unexpectedly.    \n",
        "                ‚Ä¢ You are trying to resolve a stubborn issue that persists even after a regular pip install --upgrade.   \n",
        "                    ‚Ä¢ You want to ensure you have the absolute latest code directly from the GitHub main branch, bypassing any pip caching or version comparison logic, guaranteeing a fresh pull of the most recent commits.\n",
        "                    For general updates to the latest development version where you don't suspect corruption, !pip install --upgrade git+https://github.com/openai/whisper.git is often sufficient and less aggressive. The --force-reinstall is more of a diagnostic or 'nuclear option' when other update methods aren't working as expected."
      ],
      "metadata": {
        "id": "P1CyS2eY-Kcs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7.2 **Q:** Since ‚Äìforce-reinstall negates having to upgrade or update, below upgrade code is not necessary?\n",
        "```\n",
        "pip install --upgrade --no-deps --force-reinstall git+https://github.com/openai/whisper.git\n",
        "```\n",
        "**A:** You are absolutely correct in your understanding!\n",
        "When you use pip install git+https://github.com/openai/whisper.git --force-reinstall, the --upgrade flag becomes redundant. Here's why:\n",
        "    ‚Ä¢ The --force-reinstall flag explicitly instructs pip to remove any existing installation of the package and then perform a fresh installation from the specified source (the GitHub repository in this case).\n",
        "        ‚Ä¢ Since it's a fresh installation from the current state of the repository, you are, by definition, getting the 'latest' or 'upgraded' version available at that moment directly from the source. There's no older version to \"upgrade\" from once the existing one is forced out.\n",
        "        So, while including --upgrade alongside --force-reinstall won't cause an error, it doesn't add any additional functionality or change the outcome. The --force-reinstall alone ensures you get the current code from the Git repository."
      ],
      "metadata": {
        "id": "5kohjZaJhSUC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7.3 Update Before Installing System Packages\n",
        "\n",
        "**Q:** I‚Äôve seen this update or upgrade used prior to installing something. In what scenario would update or upgrade be needed prior to installing something?\n",
        "**A:** That's a very common and important practice! You're right, running update or upgrade commands before installing something is often crucial, but the specifics depend on whether you're dealing with system-level packages (like FFmpeg) or Python packages (like Whisper or its dependencies).\n",
        "Let's break down the scenarios:\n",
        "For System-Level Packages (e.g., using apt in Colab for FFmpeg):\n",
        "Scenario: !sudo apt update before !sudo apt install <package_name>\n",
        "    ‚Ä¢ Why it's needed: When you use apt to install a system package (like ffmpeg), your system relies on a local list of available packages and their versions, stored in its cache. This list isn't always up-to-date.\n",
        "            ‚ó¶ !sudo apt update tells your system to refresh this local package list by fetching the latest information from all configured repositories on the internet. It essentially says, \"Go check what new packages, versions, and dependencies are available out there.\"\n",
        "                ‚Ä¢ When you need it: You absolutely should run !sudo apt update before !sudo apt install ffmpeg (or any other system package) if:\n",
        "                        ‚ó¶ It's your first time installing a system package in a new Colab session or environment.\n",
        "                                ‚ó¶ You want to ensure you're installing the latest stable version of the package and that all its dependencies are correctly resolved to their most recent compatible versions.\n",
        "                                        ‚ó¶ You've added new software repositories to your system (though less common in Colab).\n",
        "                                            ‚Ä¢ Consequence of not updating: If you don't update first, apt might try to install an older version of the package or might fail to find a package because its local list is outdated. It could also lead to dependency conflicts because it's working with old information.\n",
        "                                            Scenario: !sudo apt upgrade (less common before a specific install)\n",
        "                                                ‚Ä¢ Why it's used: !sudo apt upgrade takes the concept further. After update refreshes the list, upgrade then installs the newer versions of all currently installed packages on your system (not just the one you're about to install). It's for general system maintenance and ensuring everything is up-to-date.\n",
        "                                                    ‚Ä¢ When you might need it before an install: While generally not a prerequisite for installing a new specific package, you might run upgrade if you want to ensure your entire Colab environment is fully patched and has the absolute latest stable versions of all pre-installed system libraries before introducing new software. This can sometimes prevent unforeseen conflicts, but it's not strictly necessary just to install one new package.\n",
        "                                                    For Python Packages (e.g., using pip for other Python libraries):\n",
        "                                                    Scenario: pip install --upgrade <dependency_package> before pip install <main_package>\n",
        "                                                        ‚Ä¢ Why it's needed: While pip usually handles dependencies automatically, there are specific situations where you might explicitly upgrade a dependency first:\n",
        "                                                                ‚ó¶ Strict Version Requirements: If main_package explicitly states it needs dependency_package version X.Y.Z or newer, but your environment has an older version installed, pip might struggle or raise an error. Upgrading dependency_package first can sometimes resolve this.\n",
        "                                                                        ‚ó¶ Dependency Conflicts: In complex environments, one package might require dependency_package < 2.0 while another requires dependency_package >= 2.0. If you're trying to install a new package that falls into this conflict, manually upgrading a crucial dependency first (if you know it won't break other things) might be a troubleshooting step.\n",
        "                                                                                ‚ó¶ Outdated pip/setuptools: Sometimes, pip itself or the setuptools library (which pip uses) can be outdated and cause installation issues. In these rare cases, you might pip install --upgrade pip setuptools before trying to install other Python packages.\n",
        "                                                                                    ‚Ä¢ When you need it: Usually, pip install <main_package> (without --no-deps) is smart enough to figure out and install/upgrade dependencies on its own. Explicitly upgrading a dependency beforehand is typically a troubleshooting step when an initial pip install fails due to versioning issues, or when you want to force a particular dependency version.\n",
        "                                                                                    In summary:\n",
        "                                                                                        ‚Ä¢ apt update before apt install is almost always a good idea for system packages.\n",
        "                                                                                            ‚Ä¢ pip --upgrade for dependencies is usually handled automatically, but can be a manual troubleshooting step for complex Python environments or specific version conflicts."
      ],
      "metadata": {
        "id": "RQAsrWAphidp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7.4 Update Before Installing FFmpeg\n",
        "\n",
        "Q1. Before we install FFmpeg, we should do an update: sudo apt update && sudo apt install ffmpeg?\n",
        "Yes, absolutely! Running sudo apt update before sudo apt install ffmpeg (or any other apt install command for system packages) is a best practice and often crucial in environments like Google Colab. Here's why:\n",
        "    ‚Ä¢ Refreshing Package Lists: The sudo apt update command refreshes your system's local index of available software packages. This index contains information about package names, versions, and where to download them from the software repositories.\n",
        "        ‚Ä¢ Ensuring Latest Versions: If you don't run update first, apt might be working with an outdated list of packages. This could lead to it installing an older version of FFmpeg, or even failing to find the package if its name or availability has changed recently in the repositories.\n",
        "            ‚Ä¢ Dependency Resolution: An up-to-date package list helps apt correctly resolve all of FFmpeg's dependencies (other software packages it needs to function) to their latest compatible versions, preventing potential conflicts or issues.\n",
        "            The && operator ensures that sudo apt install ffmpeg only runs if sudo apt update completes successfully, which is a good safeguard."
      ],
      "metadata": {
        "id": "dW_8r5KNiN-F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. ‚ö† F5 = Refresh & will Wipe Out Gemini Chat History.\n",
        "**Q:** I did the F5 & lost our chat history. I confirm that all indicators still shows it to be a code cell based on what you had said prior to the nuclear F5 bomb: \"Here's how to confirm if the cell has indeed been converted to a text (markdown) cell:  \n",
        "       1. Look for the [ ] or [ ] next to the cell: Code cells typically have an [ ] (empty brackets) or [number] (execution count) to their left. Text cells do not have these. If these are gone, it's a text cell.   \n",
        "              2. Double-click the cell: If you double-click a text cell, it will usually switch into an editing mode where you can see the raw Markdown syntax. If you double-click a code cell, it remains in code-editing mode.   \n",
        "                     3. No 'Run cell' button: Text cells do not have a 'Play' button to run them; only code cells do.\n",
        "                     Could you please check these indicators? If it genuinely still appears as a code cell after a brief moment, you might try a quick browser refresh (F5 or Cmd/Ctrl + R) to ensure the Colab interface is fully updated. The command to change its type was sent successfully from my end."
      ],
      "metadata": {
        "id": "x6vuIurXP18V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7.5 Model ‚Äì device\n",
        "\n",
        "Do we need to run this code for Colab? Model ‚Äì device\n",
        "model.device is not a standalone command to be run, but rather an attribute (a piece of information) of a loaded Whisper model object. It's used in your notebook to tell you or set where the model is currently loaded and performing computations.\n",
        "For example, in cell P2pF5IsgsZ_N:\n",
        "#¬†make¬†log-Mel¬†spectrogram¬†and¬†move¬†to¬†the¬†same¬†device¬†as¬†the¬†model\n",
        "mel¬†=¬†whisper.log_mel_spectrogram(audio).to(model.device)\n",
        "Here, model.device returns the computational device (e.g., 'cuda' for GPU or 'cpu' for CPU) where your Whisper model (model) is loaded. The .to(model.device) part then moves the mel spectrogram (your audio data prepared for the model) to that same device. This is crucial because both the model and the input data need to be on the same device for calculations to happen efficiently and correctly. If the model is on the GPU, the data should also be on the GPU.\n",
        "So, you don't 'run' model.device as a command; you access it as a property of the model object to understand or control where computations are happening. It's essential for ensuring your model uses the available hardware accelerator (like a GPU in Colab) effectively."
      ],
      "metadata": {
        "id": "VCw2l8kJWj_M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "9t0YHQ6TfeY4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. Run ALL Codes & Turn Off the Last Landmark Code\n",
        "\n",
        "**Q:** Is there a way to run all the codes in this notebook or any .ipynb notebook?\n",
        "If yes, then is there a way to turn off one code off that sits @ the very end, specifically this code @ this notebook‚Äôs very end used for instructional purposes:\n",
        "```\n",
        "# Additional arguments: -h is a standard flag, short for --help. It's a common convention in command-line tools to display help documentation.\n",
        "!whisper¬†-h\n",
        "```\n",
        "**A:** Yes, there are ways to run all the code cells in a notebook, and also to prevent a specific cell from running.\n",
        "1. How to run all codes in a notebook:\n",
        "In Google Colab, you can easily run all the code cells in your notebook using the menu options:\n",
        "    ‚Ä¢ Go to Runtime in the top menu bar.\n",
        "        ‚Ä¢ Select Run all.\n",
        "        This will execute every code cell in your notebook sequentially, from top to bottom.\n",
        "        2. How to turn off a specific code cell:\n",
        "        To prevent the instructional !whisper -h cell from being executed when you run all cells, the most effective way is to change its cell type from 'Code' to 'Text' (Markdown). This way, its content remains visible for explanation but it will not be run as code.\n",
        "        I will modify the cell DHy2d-ysS4iL to be a text cell."
      ],
      "metadata": {
        "id": "9BEm6QKZ67Pn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8.5 Model - Device\n",
        "\n",
        "Q2, Do we need to run this code for Colab?\n",
        "```\n",
        "Model ‚Äì device\n",
        "```\n",
        "\n",
        " **A:** Do we need to run this code for Colab? Model ‚Äì device  \n",
        " model.device is not a standalone command to be run, but rather an attribute (a piece of information) of a loaded Whisper model object. It's used in your notebook to tell you or set where the model is currently loaded and performing computations.  \n",
        " For example, in cell P2pF5IsgsZ_N:  \n",
        " #¬†make¬†log-Mel¬†spectrogram¬†and¬†move¬†to¬†the¬†same¬†device¬†as¬†the¬†model\n",
        " mel¬†=¬†whisper.log_mel_spectrogram(audio).to(model.device)  \n",
        " Here, model.device returns the computational device (e.g., 'cuda' for GPU or 'cpu' for CPU) where your Whisper model (model) is loaded. The .to(model.device) part then moves the mel spectrogram (your audio data prepared for the model) to that same device. This is crucial because both the model and the input data need to be on the same device for calculations to happen efficiently and correctly. If the model is on the GPU, the data should also be on the GPU.  \n",
        " So, you don't 'run' model.device as a command; you access it as a property of the model object to understand or control where computations are happening. It's essential for ensuring your model uses the available hardware accelerator (like a GPU in Colab) effectively.  "
      ],
      "metadata": {
        "id": "qv_mn4P-XZI1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. Feynman This Final Code: (Move to Always Be @ & as the Final Code)as a Landmark\n",
        "!whisper -h  \n",
        "\n",
        "**A:** Let's break down the !whisper -h command using the Feynman technique, as if you were explaining it to someone else to truly understand it:  \n",
        "What is it? (Explain it Simply)  \n",
        "Imagine you have a powerful tool called whisper that can listen to audio and turn it into text, or even translate it. But this tool has many buttons and settings. The !whisper -h command is like pressing a big \"Help\" button on that tool. When you press it, the tool tells you all the different things it can do and all the settings (or 'arguments') you can use with it.  \n",
        "    ‚Ä¢ !: This simply tells Colab (or any Jupyter-like environment) to run the command that follows as if you were typing it directly into your computer's terminal.   \n",
        "        ‚Ä¢ whisper: This is the name of the program we're running ‚Äì the OpenAI Whisper command-line interface.   \n",
        "            ‚Ä¢ -h: This is a standard flag, short for --help. It's a common convention in command-line tools to display help documentation.\n",
        "            When to use it? (When would you need this?)\n",
        "            You'd use !whisper -h whenever you:  \n",
        "                1. Forget how to do something specific: \"How do I specify the language again?\" or \"What are the different model sizes I can use?\"   \n",
        "                    2. Want to explore features: \"What other cool things can Whisper do that I don't know about?\"    \n",
        "                        3. Are debugging an error: Sometimes an error message might mention an argument, and !whisper -h can help you understand what that argument is supposed to do or how to format it correctly.    \n",
        "                            4. Are new to the tool: It's a great first step to get an overview of all available options before diving into actual tasks.   \n",
        "                            Think of it like checking the instruction manual for your new gadget when you're trying to figure out a function.  \n",
        "                            How to use it? (How do you operate this?)   \n",
        "                            To use it, you simply type it into a code cell in Colab and run the cell:  \n",
        "                            !whisper¬†-h  \n",
        "                            When you run this, it won't actually process any audio. Instead, it will print out a long message in the output area below the cell. This message will list all the available arguments for the whisper command, along with a brief description of what each argument does, what kind of value it expects (e.g., a filename, a language code, a model name), and sometimes examples of how to use it.  \n",
        "                            For example, you'd see information about arguments like:   \n",
        "                                ‚Ä¢ audio: The path to your audio file.   \n",
        "                                    ‚Ä¢ --model: Which Whisper model to use (tiny, base, small, medium, large).   \n",
        "                                        ‚Ä¢ --language: To specify the audio's language.     \n",
        "                                            ‚Ä¢ --task: To choose between transcribe or translate.   \n",
        "                                                ‚Ä¢ --output_dir: Where to save the output files.    \n",
        "                                                By reading this output, you can then construct your actual !whisper commands with confidence, knowing all the options at your disposal.  "
      ],
      "metadata": {
        "id": "bWqhm-1pPViS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Additional arguments: -h is a standard flag, short for --help. It's a common convention in command-line tools to display help documentation.\n",
        "!whisper¬†-h"
      ],
      "metadata": {
        "id": "DHy2d-ysS4iL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}